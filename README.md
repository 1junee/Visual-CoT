# Visual CoT: Unleashing Chain-of-Thought Reasoning in the Multi-Modal Language Model

![pipeline](assets/pipeline.jpg)
       
> Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, Hongsheng Li

This repository contains code for the paper [Visual CoT: Unleashing Chain-of-Thought Reasoning in the Multi-Modal Language Model](https://github.com/deepcs233/Visual-CoT). We will release the code, pre-trained weights, dataset, and benchmark soon. 

The work proposes a multi-turn processing pipeline for the multi-modal language model that dynamically focuses on visual inputs and provides interpretable thoughts. We also collect and introduce the Visual CoT dataset comprising 373k question-answer pairs, annotated with intermediate bounding boxes highlighting key regions essential for answering the questions. Importantly, the released benchmark is capable of evaluating MLLMs in scenarios requiring specific local region identification. 


## License

All code within this repository is under [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0).
